{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d457170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tflite = tf.lite\n",
    "from math import atan2, degrees\n",
    "\n",
    "# -------------------------------\n",
    "# Utility functions\n",
    "# -------------------------------\n",
    "def draw_keypoints(frame, keypoints, threshold=0.3):\n",
    "    h, w, _ = frame.shape\n",
    "    for y, x, score in keypoints:\n",
    "        if score > threshold:\n",
    "            cv2.circle(frame, (int(x * w), int(y * h)), 5, (0, 255, 0), -1)\n",
    "\n",
    "def draw_connections(frame, keypoints, edges, threshold=0.3):\n",
    "    h, w, _ = frame.shape\n",
    "    for (start, end) in edges:\n",
    "        y1, x1, s1 = keypoints[start]\n",
    "        y2, x2, s2 = keypoints[end]\n",
    "        if s1 > threshold and s2 > threshold:\n",
    "            cv2.line(frame, (int(x1*w), int(y1*h)), (int(x2*w), int(y2*h)), (0,255,255), 2)\n",
    "\n",
    "def calculate_angle(a, b):\n",
    "    \"\"\"Calculate angle between two points (a,b) in degrees relative to vertical\"\"\"\n",
    "    dy = a[1] - b[1]\n",
    "    dx = a[0] - b[0]\n",
    "    return degrees(atan2(dy, dx))\n",
    "\n",
    "def midpoint(a, b):\n",
    "    return ((a[0]+b[0])/2, (a[1]+b[1])/2)\n",
    "\n",
    "def draw_line(frame, pt1, pt2, color=(255,0,0), thickness=2, label=\"\"):\n",
    "    h, w, _ = frame.shape\n",
    "    pt1_pixel = (int(pt1[0]*w), int(pt1[1]*h))\n",
    "    pt2_pixel = (int(pt2[0]*w), int(pt2[1]*h))\n",
    "    cv2.line(frame, pt1_pixel, pt2_pixel, color, thickness)\n",
    "    if label:\n",
    "        cv2.putText(frame, label, (pt2_pixel[0]+5, pt2_pixel[1]+5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "# -------------------------------\n",
    "# Load TFLite MoveNet model\n",
    "# -------------------------------\n",
    "interpreter = tflite.Interpreter(model_path=\"movenet-tflite-singlepose-lightning-v1.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# -------------------------------\n",
    "# Skeleton edges\n",
    "# -------------------------------\n",
    "EDGES = [\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "    (0, 5), (0, 6),\n",
    "    (5, 7), (7, 9),\n",
    "    (6, 8), (8,10),\n",
    "    (5,11), (6,12),\n",
    "    (11,12),\n",
    "    (11,13), (13,15),\n",
    "    (12,14), (14,16)\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# Video capture\n",
    "# -------------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img, (192,192))\n",
    "    input_image = np.expand_dims(img_resized.astype(np.float32), axis=0)\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image)\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])[0][0]\n",
    "\n",
    "    draw_connections(frame, keypoints_with_scores, EDGES)\n",
    "    draw_keypoints(frame, keypoints_with_scores)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Posture detection\n",
    "    # -------------------------------\n",
    "    nose = keypoints_with_scores[0][:2]\n",
    "    left_shoulder = keypoints_with_scores[5][:2]\n",
    "    right_shoulder = keypoints_with_scores[6][:2]\n",
    "    left_hip = keypoints_with_scores[11][:2]\n",
    "    right_hip = keypoints_with_scores[12][:2]\n",
    "\n",
    "    shoulder_mid = midpoint(left_shoulder, right_shoulder)\n",
    "    hip_mid = midpoint(left_hip, right_hip)\n",
    "\n",
    "    torso_angle = calculate_angle(hip_mid, shoulder_mid)\n",
    "    neck_angle = calculate_angle(shoulder_mid, nose)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Draw angle lines\n",
    "    draw_line(frame, shoulder_mid, nose, color=(0,0,255), label=f\"Neck {neck_angle:.1f} deg\")\n",
    "    draw_line(frame, hip_mid, shoulder_mid, color=(255,0,0), label=f\"Torso {torso_angle:.1f} deg\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Good/Bad posture thresholds\n",
    "    # (You can adjust these)\n",
    "    torso_good = abs(torso_angle) < 10       # near vertical\n",
    "    neck_good = abs(neck_angle) < 20         # slight forward tilt allowed\n",
    "\n",
    "    posture_text = \"GOOD\" if torso_good and neck_good else \"BAD\"\n",
    "    color = (0,255,0) if posture_text==\"GOOD\" else (0,0,255)\n",
    "    cv2.putText(frame, f\"Posture: {posture_text}\", (10,90),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    # -------------------------------\n",
    "    cv2.imshow(\"Posture Tracking\", frame)\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
