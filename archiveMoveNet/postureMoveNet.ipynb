{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f1313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tflite = tf.lite\n",
    "from math import atan2, degrees\n",
    "\n",
    "# -------------------------------\n",
    "# Utility functions\n",
    "# -------------------------------\n",
    "def draw_keypoints(frame, keypoints, threshold=0.3):\n",
    "    h, w, _ = frame.shape\n",
    "    for y, x, score in keypoints:\n",
    "        if score > threshold:\n",
    "            cv2.circle(frame, (int(x * w), int(y * h)), 5, (0, 255, 0), -1)\n",
    "\n",
    "def draw_connections(frame, keypoints, edges, threshold=0.3):\n",
    "    h, w, _ = frame.shape\n",
    "    for (start, end) in edges:\n",
    "        y1, x1, s1 = keypoints[start]\n",
    "        y2, x2, s2 = keypoints[end]\n",
    "        if s1 > threshold and s2 > threshold:\n",
    "            cv2.line(frame, (int(x1*w), int(y1*h)), (int(x2*w), int(y2*h)), (0,255,255), 2)\n",
    "\n",
    "def calculate_angle(a, b):\n",
    "    \"\"\"Calculate angle between two points (a,b) in degrees relative to vertical\"\"\"\n",
    "    dy = a[1] - b[1]\n",
    "    dx = a[0] - b[0]\n",
    "    return degrees(atan2(dy, dx))\n",
    "\n",
    "# -------------------------------\n",
    "# Load TFLite MoveNet model\n",
    "# -------------------------------\n",
    "interpreter = tflite.Interpreter(model_path=\"movenet-tflite-singlepose-lightning-v1.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# -------------------------------\n",
    "# Define skeleton edges\n",
    "# -------------------------------\n",
    "# Keypoint indices from MoveNet\n",
    "# 0: nose, 1:left_eye, 2:right_eye, 3:left_ear, 4:right_ear\n",
    "# 5:left_shoulder, 6:right_shoulder, 7:left_elbow, 8:right_elbow\n",
    "# 9:left_wrist, 10:right_wrist, 11:left_hip, 12:right_hip\n",
    "# 13:left_knee, 14:right_knee, 15:left_ankle, 16:right_ankle\n",
    "EDGES = [\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "    (0, 5), (0, 6),\n",
    "    (5, 7), (7, 9),\n",
    "    (6, 8), (8,10),\n",
    "    (5,11), (6,12),\n",
    "    (11,12),\n",
    "    (11,13), (13,15),\n",
    "    (12,14), (14,16)\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# Capture video\n",
    "# -------------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img, (192,192))\n",
    "    input_image = np.expand_dims(img_resized.astype(np.float32), axis=0)\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image)\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])[0][0]\n",
    "\n",
    "    # keypoints_with_scores: 17x3 (y, x, score)\n",
    "    draw_connections(frame, keypoints_with_scores, EDGES)\n",
    "    draw_keypoints(frame, keypoints_with_scores)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Posture detection: neck & torso angle\n",
    "    # -------------------------------\n",
    "    # Approximate points\n",
    "    nose = keypoints_with_scores[0][:2]\n",
    "    left_shoulder = keypoints_with_scores[5][:2]\n",
    "    right_shoulder = keypoints_with_scores[6][:2]\n",
    "    left_hip = keypoints_with_scores[11][:2]\n",
    "    right_hip = keypoints_with_scores[12][:2]\n",
    "\n",
    "    # Midpoints\n",
    "    shoulder_mid = ((left_shoulder[0]+right_shoulder[0])/2, (left_shoulder[1]+right_shoulder[1])/2)\n",
    "    hip_mid = ((left_hip[0]+right_hip[0])/2, (left_hip[1]+right_hip[1])/2)\n",
    "\n",
    "    # Torso angle\n",
    "    torso_angle = calculate_angle(hip_mid, shoulder_mid)  # relative to vertical\n",
    "\n",
    "    # Neck angle (shoulder -> nose)\n",
    "    neck_angle = calculate_angle(shoulder_mid, nose)\n",
    "\n",
    "    cv2.putText(frame, f\"Torso angle: {torso_angle:.1f} deg\", (10,30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)\n",
    "    cv2.putText(frame, f\"Neck angle: {neck_angle:.1f} deg\", (10,60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)\n",
    "\n",
    "    # -------------------------------\n",
    "    cv2.imshow(\"Posture Tracking\", frame)\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
